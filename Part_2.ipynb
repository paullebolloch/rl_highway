{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf145df",
   "metadata": {},
   "source": [
    "# Part 2 : Implementation of a continuous environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58b4f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "import highway_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee6de2",
   "metadata": {},
   "source": [
    "### 1. Load environment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "afc50a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\Utilisateur\\iCloudDrive\\Travail cs\\IA\\RL\\Projet\\rl_highway\\videos_racetrack folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"config_part2.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "# Create the continuous-action Racetrack environment\n",
    "base_env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\")\n",
    "base_env.unwrapped.configure(config)\n",
    "# Record stats and videos: record 20 videos uniformly across training\n",
    "env = RecordEpisodeStatistics(base_env)\n",
    "env = RecordVideo(env,\n",
    "                   video_folder=\"videos_racetrack/\",\n",
    "                   episode_trigger=lambda episode_id: episode_id % 20 == 0,\n",
    "                   name_prefix=\"train_ep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d76783a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c37f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99            # discount factor\n",
    "tau = 0.005             # target network update rate\n",
    "actor_lr = 1e-4         # learning rate for actor\n",
    "critic_lr = 1e-3        # learning rate for critic\n",
    "buffer_capacity = 100000\n",
    "batch_size = 64\n",
    "num_episodes = 20\n",
    "max_steps = config.get(\"duration\") * config.get(\"policy_frequency\")\n",
    "writer = SummaryWriter(log_dir='racetrack_dqn/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57313c3b",
   "metadata": {},
   "source": [
    "### 2. Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ecc9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append((state, action, reward, done, next_state))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, dones, next_states = map(np.stack, zip(*batch))\n",
    "        return states, actions, rewards, dones, next_states\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8ca69",
   "metadata": {},
   "source": [
    "### 3. Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d8ceb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()  # output in [-1,1]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c48b6f",
   "metadata": {},
   "source": [
    "### 4. Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25bd2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7143f",
   "metadata": {},
   "source": [
    "### 5. DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b6fd457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise:\n",
    "    def __init__(self, dim, std):\n",
    "        self.dim = dim\n",
    "        self.std = std\n",
    "    def sample(self):\n",
    "        # retourne un vecteur de taille dim ~ N(0, std²)\n",
    "        return np.random.randn(self.dim).astype(np.float32) * self.std\n",
    "    \n",
    "class DDPGAgent:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        # Main networks\n",
    "        self.actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.critic = Critic(obs_dim, act_dim).to(device)\n",
    "        # Target networks\n",
    "        self.targ_actor = Actor(obs_dim, act_dim).to(device)\n",
    "        self.targ_critic = Critic(obs_dim, act_dim).to(device)\n",
    "        # Copy weights\n",
    "        self.targ_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.targ_critic.load_state_dict(self.critic.state_dict())\n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        # Replay buffer\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        # Exploration noise std\n",
    "        self.noise_std = 0.1\n",
    "        self.noise = GaussianNoise(act_dim, self.noise_std)\n",
    "\n",
    "        self.act_low, self.act_high = env.action_space.low, env.action_space.high\n",
    "    \n",
    "    def get_action(self, state, noise=True):\n",
    "        # 0) Flatten\n",
    "        state_arr = np.array(state, dtype=np.float32).ravel()\n",
    "        # 1) Batch dim\n",
    "        state_t = torch.from_numpy(state_arr).unsqueeze(0).to(device)\n",
    "\n",
    "        # 2) Prédiction de l’Actor et extraction de la batch 0\n",
    "        action = self.actor(state_t).cpu().detach().numpy()[0]\n",
    "\n",
    "        # 4) Exploration\n",
    "        if noise:\n",
    "            action += self.noise.sample()\n",
    "\n",
    "        # 5) Remise à l’échelle de [−1,1] → [act_low, act_high] + clipping\n",
    "        action = np.clip(\n",
    "            action * (self.act_high - self.act_low) + self.act_low,\n",
    "            self.act_low,\n",
    "            self.act_high\n",
    "        )\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self, step):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        # Sample a batch\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        \n",
    "        low = env.action_space.low\n",
    "        high = env.action_space.high\n",
    "\n",
    "        # Critic loss\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.targ_actor(next_states)\n",
    "            next_actions = low + (next_actions + 1.0) * 0.5 * (high - low)\n",
    "            target_q = self.targ_critic(next_states, next_actions)\n",
    "            y = rewards + gamma * (1 - dones) * target_q\n",
    "        q = self.critic(states, actions)\n",
    "        critic_loss = nn.MSELoss()(q, y)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor loss (maximize Q)\n",
    "        pred_actions = self.actor(states)\n",
    "        pred_actions = low + (pred_actions + 1.0) * 0.5 * (high - low)\n",
    "        actor_loss = -self.critic(states, pred_actions).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft update target networks\n",
    "        for param, targ_param in zip(self.actor.parameters(), self.targ_actor.parameters()):\n",
    "            targ_param.data.copy_(tau * param.data + (1 - tau) * targ_param.data)\n",
    "        for param, targ_param in zip(self.critic.parameters(), self.targ_critic.parameters()):\n",
    "            targ_param.data.copy_(tau * param.data + (1 - tau) * targ_param.data)\n",
    "\n",
    "                # Log losses to TensorBoard\n",
    "        writer.add_scalar('loss/critic', critic_loss.item(), step)\n",
    "        writer.add_scalar('loss/actor', actor_loss.item(), step)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, done, next_state):\n",
    "        self.buffer.push(state, action, reward, done, next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd505c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat.shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()  \n",
    "flat = np.array(obs, dtype=np.float32).ravel()\n",
    "print(\"flat.shape:\", flat.shape)   # ← doit être (6,) si vos next_states sont en 6 colonnes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f4018",
   "metadata": {},
   "source": [
    "### 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65afc869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_obs.shape = (30,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (320x6 and 30x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     17\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, done, next_state)\n\u001b[1;32m---> 18\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     20\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36mDDPGAgent.update\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Critic loss\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 69\u001b[0m     next_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarg_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     next_actions \u001b[38;5;241m=\u001b[39m low \u001b[38;5;241m+\u001b[39m (next_actions \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (high \u001b[38;5;241m-\u001b[39m low)\n\u001b[0;32m     71\u001b[0m     target_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarg_critic(next_states, next_actions)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mActor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (320x6 and 30x256)"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()            # gymnasium retourne (obs, info)\n",
    "flat_obs = np.array(obs, dtype=np.float32).ravel()\n",
    "print(\"flat_obs.shape =\", flat_obs.shape)   # disons (30,)\n",
    "\n",
    "obs_dim = flat_obs.shape[0]     # 30\n",
    "act_dim = env.action_space.shape[0]\n",
    "agent = DDPGAgent(obs_dim, act_dim)\n",
    "\n",
    "global_step = 0\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0.0\n",
    "    for step in range(int(max_steps)):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.store_transition(state, action, reward, done, next_state)\n",
    "        agent.update(global_step)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        global_step += 1\n",
    "        if done:\n",
    "            break\n",
    "    # Log episode reward\n",
    "    writer.add_scalar('episode/reward', total_reward, ep)\n",
    "    print(f\"Episode {ep+1}/{num_episodes}: Reward = {total_reward:.2f}\")\n",
    "\n",
    "# Save final models\n",
    "torch.save(agent.actor.state_dict(), \"ddpg_actor.pth\")\n",
    "torch.save(agent.critic.state_dict(), \"ddpg_critic.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ccce1",
   "metadata": {},
   "source": [
    "### 7. Evaluation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeac190",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = RecordEpisodeStatistics(\n",
    "    gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\").unwrapped.configure(config)\n",
    ")\n",
    "# Test trained agent\n",
    "trained_rewards = []\n",
    "agent.actor.eval()\n",
    "for i in range(10):\n",
    "    state, _ = eval_env.reset()\n",
    "    r_sum = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = agent.get_action(state, noise=False)\n",
    "        state, r, term, trunc, _ = eval_env.step(a)\n",
    "        r_sum += r\n",
    "        done = term or trunc\n",
    "    trained_rewards.append(r_sum)\n",
    "# Test untrained (random) agent\n",
    "random_rewards = []\n",
    "for i in range(10):\n",
    "    state, _ = eval_env.reset()\n",
    "    r_sum = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = eval_env.action_space.sample()\n",
    "        state, r, term, trunc, _ = eval_env.step(a)\n",
    "        r_sum += r\n",
    "        done = term or trunc\n",
    "    random_rewards.append(r_sum)\n",
    "\n",
    "print(\"Average trained reward:\", np.mean(trained_rewards))\n",
    "print(\"Average random reward:\", np.mean(random_rewards))\n",
    "# Close environments\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d096ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
