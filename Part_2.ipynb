{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d01df9",
   "metadata": {},
   "source": [
    "# Partie 2 : Implémentation d'un algorithme DDPG sur un environnement continu Racetrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cfe57dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo, FlattenObservation\n",
    "import highway_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e495c",
   "metadata": {},
   "source": [
    "### 1. Chargement de la configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8b1bab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dim = 288, action_dim = 1\n",
      "→ state_dim = 288, action_dim = 1\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Chargez la config pickle ---\n",
    "with open(\"config_part2.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "# --- 2. Créez l'env et configurer la vraie instance ---\n",
    "env = gym.make(\"racetrack-v0\", render_mode='rgb_array')\n",
    "raw_env = env.unwrapped          # on récupère l'instance sans wrappers\n",
    "raw_env.configure(config)        # applique la config\n",
    "\n",
    "# 3. Aplatissez l'observation (de (5,6) → (30,))\n",
    "flat_env = FlattenObservation(raw_env)\n",
    "\n",
    "\n",
    "# --- 3. Re-wrapper pour n'enregistrer qu'un épisode sur 10 ---\n",
    "env = RecordVideo(\n",
    "    flat_env,\n",
    "    video_folder=\"videos_racetrack_grid/\",\n",
    "    name_prefix=\"ddpg-racetrack_occupancy\",\n",
    "    episode_trigger=lambda episode_id: episode_id % 10 == 0\n",
    ")\n",
    "\n",
    "# 4. Infère state_dim & action_dim dynamiquement\n",
    "obs, _ = env.reset()\n",
    "state = obs if not isinstance(obs, dict) else obs[\"observation\"]\n",
    "state_dim  = state.shape[0]               # devrait être 12*12*2 = 288\n",
    "action_dim = env.action_space.shape[0]    # steering seul → 1\n",
    "act_max    = env.action_space.high\n",
    "\n",
    "print(f\"state_dim = {state_dim}, action_dim = {action_dim}\")\n",
    "\n",
    "\n",
    "### Pour l'environnement kinematics ###\n",
    "# 5. Récupère un obs réel pour inférer state_dim\n",
    "# reset_out = env.reset()\n",
    "\n",
    "# # gymnasium renvoie (obs, info)\n",
    "# obs = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
    "# # si c’est un dict, prendre la clé \"observation\"\n",
    "# if isinstance(obs, dict):\n",
    "#     obs = obs[\"observation\"]\n",
    "  \n",
    "# state_dim  = obs.shape[0]                # = 30 après flatten\n",
    "# action_dim = env.action_space.shape[0]   # = 2\n",
    "# act_max    = env.action_space.high\n",
    "\n",
    "print(f\"→ state_dim = {state_dim}, action_dim = {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512e588",
   "metadata": {},
   "source": [
    "### 2. Réseaux Actor / Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0d627fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, a_max):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(s_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        # self.fc3 = nn.Linear(256, a_dim)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.a_max = torch.tensor(a_max, dtype=torch.float32)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # tanh ∈ [-1,1], on remet à l’échelle\n",
    "        return torch.tanh(self.fc3(x)) * self.a_max\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(s_dim + a_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69c533",
   "metadata": {},
   "source": [
    "### 3. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "984a19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=int(1e6)):\n",
    "        self.storage = deque(maxlen=max_size)\n",
    "    def add(self, data):\n",
    "        self.storage.append(data)\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.storage, batch_size)\n",
    "        s, a, r, s2, d = zip(*batch)\n",
    "        return (np.vstack(s), np.vstack(a), np.vstack(r), np.vstack(s2), np.vstack(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c725c13",
   "metadata": {},
   "source": [
    "### 4. Bruit Ornstein–Uhlenbeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "16181232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta, self.sigma = theta, sigma\n",
    "        self.state = self.mu.copy()\n",
    "    def reset(self):\n",
    "        self.state = self.mu.copy()\n",
    "    def __call__(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(len(self.state))\n",
    "        self.state += dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219dde6e",
   "metadata": {},
   "source": [
    "### 5. Agent DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f0a627eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, s_dim, a_dim, a_max, device):\n",
    "        self.device = device\n",
    "        # réseaux principaux et cibles\n",
    "        self.actor = Actor(s_dim, a_dim, a_max).to(device)\n",
    "        self.actor_target = Actor(s_dim, a_dim, a_max).to(device)\n",
    "        self.critic = Critic(s_dim, a_dim).to(device)\n",
    "        self.critic_target = Critic(s_dim, a_dim).to(device)\n",
    "        # synchronisation initiale\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        # optimizers\n",
    "        self.opt_a = optim.Adam(self.actor.parameters(), lr=1e-5)\n",
    "        self.opt_c = optim.Adam(self.critic.parameters(), lr=5e-4)\n",
    "        # buffer & bruit\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.noise = OUNoise(a_dim)\n",
    "        # hyperparams\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 5e-4\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def select_action(self, state, noise=True):\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action = self.actor(state_t).cpu().data.numpy().flatten()\n",
    "        if noise:\n",
    "            action += self.noise()\n",
    "        return np.clip(action, -act_max, act_max)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer.storage) < self.batch_size:\n",
    "            return None, None\n",
    "        s, a, r, s2, done = self.buffer.sample(self.batch_size)\n",
    "        s = torch.FloatTensor(s).to(self.device)\n",
    "        a = torch.FloatTensor(a).to(self.device)\n",
    "        r = torch.FloatTensor(r).to(self.device)\n",
    "        s2 = torch.FloatTensor(s2).to(self.device)\n",
    "        done = torch.FloatTensor(1 - done).to(self.device)\n",
    "\n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            a2 = self.actor_target(s2)\n",
    "            q2 = self.critic_target(s2, a2)\n",
    "            q_target = r + self.gamma * done * q2\n",
    "        q = self.critic(s, a)\n",
    "        loss_c = F.mse_loss(q, q_target)\n",
    "        self.opt_c.zero_grad(); loss_c.backward(); self.opt_c.step()\n",
    "\n",
    "        # Actor update\n",
    "        loss_a = -self.critic(s, self.actor(s)).mean()\n",
    "        self.opt_a.zero_grad(); loss_a.backward(); self.opt_a.step()\n",
    "\n",
    "        # Soft update cibles\n",
    "        for param, target in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target.data.copy_(self.tau * param.data + (1 - self.tau) * target.data)\n",
    "        for param, target in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target.data.copy_(self.tau * param.data + (1 - self.tau) * target.data)\n",
    "\n",
    "        return loss_a.item(), loss_c.item()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.actor.state_dict(), os.path.join(path, \"actor.pth\"))\n",
    "        torch.save(self.critic.state_dict(), os.path.join(path, \"critic.pth\"))\n",
    "\n",
    "    def load(self, path):\n",
    "        self.actor.load_state_dict(torch.load(os.path.join(path, \"actor.pth\")))\n",
    "        self.critic.load_state_dict(torch.load(os.path.join(path, \"critic.pth\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f216d30",
   "metadata": {},
   "source": [
    "### 6. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1aa36fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(agent, env, max_episodes=200, max_steps=150, save_dir=\"models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    writer = SummaryWriter()\n",
    "    rewards_history = []\n",
    "\n",
    "    \n",
    "    for ep in range(1, max_episodes + 1):\n",
    "        # gymnasium.reset() → (obs, info)\n",
    "        reset_out = env.reset()\n",
    "        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
    "        # state est un vecteur [x,y,vx,vy,cos_h,sin_h]\n",
    "        cx, cy = env.unwrapped.config[\"centering_position\"]\n",
    "        if isinstance(state, dict):\n",
    "            state = state.get(\"observation\", next(iter(state.values())))\n",
    "        agent.noise.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_loss_a   = 0.0\n",
    "        ep_loss_c   = 0.0\n",
    "        update_count= 0\n",
    "        collisions  = 0\n",
    "        dist_center = 0.0\n",
    "        actions     = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            actions.append(action)\n",
    "\n",
    "            step_out = env.step(action)\n",
    "            # gymnasium.step() → (obs, reward, terminated, truncated, info)\n",
    "            if len(step_out) == 5:\n",
    "                next_obs, reward, term, trunc, info = step_out\n",
    "                done = term or trunc\n",
    "            else:\n",
    "                next_obs, reward, done, info = step_out\n",
    "            if isinstance(next_obs, tuple):\n",
    "                next_obs = next_obs[0]\n",
    "            if isinstance(next_obs, dict):\n",
    "                next_obs = next_obs.get(\"observation\", next(iter(next_obs.values())))\n",
    "\n",
    "            # 4) Metrics d'environnement\n",
    "            collisions  += info.get(\"collision_count\", 0)\n",
    "            x,y = state[0], state[1]\n",
    "            dist_center += ((x - cx)**2 + (y - cy)**2)**0.5\n",
    "\n",
    "            agent.buffer.add((state, action, reward, next_obs, float(done)))\n",
    "            state = next_obs\n",
    "            ep_reward += reward\n",
    "\n",
    "            la, lc = agent.update()\n",
    "\n",
    "            if la is not None:\n",
    "                ep_loss_a    += la\n",
    "                ep_loss_c    += lc\n",
    "                update_count += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # ==== Post-épisode ====\n",
    "        length = step + 1\n",
    "        # moyennes de loss (si jamais pas de maj, on évite la division par 0)\n",
    "        avg_loss_a = ep_loss_a / update_count if update_count else 0.0\n",
    "        avg_loss_c = ep_loss_c / update_count if update_count else 0.0\n",
    "\n",
    "        # stats d'action\n",
    "        actions_arr = np.array(actions)  # shape (length, action_dim)\n",
    "        mean_steer       = actions_arr[:, 0].mean()\n",
    "        std_steer        = actions_arr[:, 0].std()\n",
    "        mean_acceleration= actions_arr[:, 1].mean() if actions_arr.shape[1] > 1 else 0.0\n",
    "        std_acceleration = actions_arr[:, 1].std()  if actions_arr.shape[1] > 1 else 0.0\n",
    "\n",
    "        # ==== Logging TensorBoard ====\n",
    "        writer.add_scalar(\"Reward/episode\",        ep_reward,      ep)\n",
    "        writer.add_scalar(\"Loss/actor\",            avg_loss_a,     ep)\n",
    "        writer.add_scalar(\"Loss/critic\",           avg_loss_c,     ep)\n",
    "        writer.add_scalar(\"Episode/length\",        length,         ep)\n",
    "        writer.add_scalar(\"Env/collisions\",        collisions,     ep)\n",
    "        writer.add_scalar(\"Env/avg_dist_center\",   dist_center/length, ep)\n",
    "\n",
    "        writer.add_scalar(\"Action/mean_steer\",         mean_steer,        ep)\n",
    "        writer.add_scalar(\"Action/std_steer\",          std_steer,         ep)\n",
    "        writer.add_scalar(\"Action/mean_acceleration\",  mean_acceleration, ep)\n",
    "        writer.add_scalar(\"Action/std_acceleration\",   std_acceleration,  ep)\n",
    "\n",
    "        # console & save\n",
    "        print(f\"[Episode {ep:03d}] Reward: {ep_reward:.2f} | Length: {length}\")\n",
    "        if ep % 50 == 0:\n",
    "            agent.save(save_dir)\n",
    "\n",
    "        rewards_history.append(ep_reward)\n",
    "\n",
    "    # Fin d’entraînement : sauvegarde finale, fermeture\n",
    "    agent.save(save_dir)\n",
    "    writer.close()\n",
    "    env.close()   # <— IMPORTANT pour forcer l’écriture du dernier .mp4\n",
    "\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb009b",
   "metadata": {},
   "source": [
    "### 8. Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "28c09dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 001] Reward: 6.04 | Length: 150\n",
      "[Episode 002] Reward: 3.18 | Length: 150\n",
      "[Episode 003] Reward: 26.35 | Length: 150\n",
      "[Episode 004] Reward: 28.63 | Length: 150\n",
      "[Episode 005] Reward: 45.78 | Length: 150\n",
      "[Episode 006] Reward: 55.09 | Length: 150\n",
      "[Episode 007] Reward: 42.28 | Length: 150\n",
      "[Episode 008] Reward: 7.72 | Length: 44\n",
      "[Episode 009] Reward: 50.45 | Length: 150\n",
      "[Episode 010] Reward: 12.68 | Length: 150\n",
      "[Episode 011] Reward: 42.98 | Length: 150\n",
      "[Episode 012] Reward: 30.26 | Length: 150\n",
      "[Episode 013] Reward: 31.29 | Length: 150\n",
      "[Episode 014] Reward: 43.84 | Length: 150\n",
      "[Episode 015] Reward: 35.81 | Length: 150\n",
      "[Episode 016] Reward: 13.69 | Length: 150\n",
      "[Episode 017] Reward: 5.27 | Length: 150\n",
      "[Episode 018] Reward: 1.87 | Length: 150\n",
      "[Episode 019] Reward: 91.19 | Length: 150\n",
      "[Episode 020] Reward: 34.60 | Length: 53\n",
      "[Episode 021] Reward: 99.60 | Length: 150\n",
      "[Episode 022] Reward: 27.31 | Length: 52\n",
      "[Episode 023] Reward: 110.78 | Length: 150\n",
      "[Episode 024] Reward: 109.83 | Length: 150\n",
      "[Episode 025] Reward: 102.61 | Length: 150\n",
      "[Episode 026] Reward: 96.70 | Length: 150\n",
      "[Episode 027] Reward: 95.28 | Length: 150\n",
      "[Episode 028] Reward: 107.40 | Length: 149\n",
      "[Episode 029] Reward: 118.16 | Length: 150\n",
      "[Episode 030] Reward: 59.40 | Length: 75\n",
      "[Episode 031] Reward: 16.54 | Length: 150\n",
      "[Episode 032] Reward: 44.21 | Length: 65\n",
      "[Episode 033] Reward: 106.23 | Length: 150\n",
      "[Episode 034] Reward: 111.18 | Length: 150\n",
      "[Episode 035] Reward: 47.79 | Length: 74\n",
      "[Episode 036] Reward: 30.45 | Length: 38\n",
      "[Episode 037] Reward: 47.02 | Length: 62\n",
      "[Episode 038] Reward: 103.09 | Length: 150\n",
      "[Episode 039] Reward: 109.60 | Length: 150\n",
      "[Episode 040] Reward: 73.63 | Length: 150\n",
      "[Episode 041] Reward: 96.46 | Length: 150\n",
      "[Episode 042] Reward: 113.01 | Length: 150\n",
      "[Episode 043] Reward: 38.62 | Length: 54\n",
      "[Episode 044] Reward: 90.27 | Length: 150\n",
      "[Episode 045] Reward: 56.75 | Length: 150\n",
      "[Episode 046] Reward: 64.44 | Length: 150\n",
      "[Episode 047] Reward: 37.68 | Length: 53\n",
      "[Episode 048] Reward: 85.08 | Length: 150\n",
      "[Episode 049] Reward: 115.18 | Length: 150\n",
      "[Episode 050] Reward: 87.38 | Length: 150\n",
      "[Episode 051] Reward: 89.55 | Length: 150\n",
      "[Episode 052] Reward: 42.06 | Length: 55\n",
      "[Episode 053] Reward: 105.16 | Length: 150\n",
      "[Episode 054] Reward: 110.93 | Length: 150\n",
      "[Episode 055] Reward: 105.40 | Length: 150\n",
      "[Episode 056] Reward: 66.83 | Length: 121\n",
      "[Episode 057] Reward: 25.45 | Length: 150\n",
      "[Episode 058] Reward: 35.13 | Length: 150\n",
      "[Episode 059] Reward: 107.31 | Length: 150\n",
      "[Episode 060] Reward: 60.14 | Length: 80\n",
      "[Episode 061] Reward: 80.53 | Length: 121\n",
      "[Episode 062] Reward: 30.64 | Length: 37\n",
      "[Episode 063] Reward: 93.07 | Length: 150\n",
      "[Episode 064] Reward: 108.92 | Length: 150\n",
      "[Episode 065] Reward: 90.34 | Length: 150\n",
      "[Episode 066] Reward: 89.47 | Length: 150\n",
      "[Episode 067] Reward: 57.59 | Length: 71\n",
      "[Episode 068] Reward: 109.47 | Length: 150\n",
      "[Episode 069] Reward: 24.53 | Length: 39\n",
      "[Episode 070] Reward: 113.15 | Length: 150\n",
      "[Episode 071] Reward: 107.17 | Length: 150\n",
      "[Episode 072] Reward: 113.45 | Length: 150\n",
      "[Episode 073] Reward: 107.00 | Length: 150\n",
      "[Episode 074] Reward: 63.62 | Length: 150\n",
      "[Episode 075] Reward: 29.99 | Length: 40\n",
      "[Episode 076] Reward: 27.48 | Length: 39\n",
      "[Episode 077] Reward: 68.30 | Length: 95\n",
      "[Episode 078] Reward: 15.95 | Length: 23\n",
      "[Episode 079] Reward: 103.28 | Length: 150\n",
      "[Episode 080] Reward: 50.41 | Length: 66\n",
      "[Episode 081] Reward: 32.29 | Length: 40\n",
      "[Episode 082] Reward: 35.83 | Length: 51\n",
      "[Episode 083] Reward: 30.38 | Length: 40\n",
      "[Episode 084] Reward: 110.74 | Length: 150\n",
      "[Episode 085] Reward: 70.61 | Length: 97\n",
      "[Episode 086] Reward: 92.58 | Length: 135\n",
      "[Episode 087] Reward: 104.48 | Length: 150\n",
      "[Episode 088] Reward: 89.67 | Length: 150\n",
      "[Episode 089] Reward: 99.91 | Length: 133\n",
      "[Episode 090] Reward: 46.74 | Length: 59\n",
      "[Episode 091] Reward: 41.04 | Length: 57\n",
      "[Episode 092] Reward: 112.34 | Length: 150\n",
      "[Episode 093] Reward: 15.37 | Length: 23\n",
      "[Episode 094] Reward: 102.96 | Length: 150\n",
      "[Episode 095] Reward: 116.41 | Length: 150\n",
      "[Episode 096] Reward: 65.12 | Length: 89\n",
      "[Episode 097] Reward: 71.42 | Length: 97\n",
      "[Episode 098] Reward: 68.31 | Length: 105\n",
      "[Episode 099] Reward: 119.20 | Length: 150\n",
      "[Episode 100] Reward: 56.86 | Length: 69\n",
      "[Episode 101] Reward: 107.42 | Length: 150\n",
      "[Episode 102] Reward: 39.73 | Length: 59\n",
      "[Episode 103] Reward: 57.16 | Length: 79\n",
      "[Episode 104] Reward: 44.04 | Length: 56\n",
      "[Episode 105] Reward: 116.67 | Length: 150\n",
      "[Episode 106] Reward: 104.29 | Length: 150\n",
      "[Episode 107] Reward: 102.17 | Length: 150\n",
      "[Episode 108] Reward: 48.96 | Length: 67\n",
      "[Episode 109] Reward: 115.17 | Length: 150\n",
      "[Episode 110] Reward: 113.88 | Length: 150\n",
      "[Episode 111] Reward: 43.52 | Length: 67\n",
      "[Episode 112] Reward: 82.70 | Length: 150\n",
      "[Episode 113] Reward: 38.26 | Length: 52\n",
      "[Episode 114] Reward: 110.62 | Length: 150\n",
      "[Episode 115] Reward: 44.61 | Length: 58\n",
      "[Episode 116] Reward: 48.36 | Length: 71\n",
      "[Episode 117] Reward: 93.76 | Length: 150\n",
      "[Episode 118] Reward: 17.70 | Length: 24\n",
      "[Episode 119] Reward: 41.35 | Length: 75\n",
      "[Episode 120] Reward: 115.38 | Length: 150\n",
      "[Episode 121] Reward: 100.70 | Length: 150\n",
      "[Episode 122] Reward: 111.34 | Length: 150\n",
      "[Episode 123] Reward: 101.56 | Length: 127\n",
      "[Episode 124] Reward: 20.03 | Length: 28\n",
      "[Episode 125] Reward: 102.57 | Length: 150\n",
      "[Episode 126] Reward: 109.55 | Length: 150\n",
      "[Episode 127] Reward: 106.67 | Length: 150\n",
      "[Episode 128] Reward: 104.51 | Length: 150\n",
      "[Episode 129] Reward: 27.64 | Length: 41\n",
      "[Episode 130] Reward: 74.28 | Length: 108\n",
      "[Episode 131] Reward: 60.60 | Length: 82\n",
      "[Episode 132] Reward: 57.46 | Length: 76\n",
      "[Episode 133] Reward: 91.89 | Length: 150\n",
      "[Episode 134] Reward: 108.29 | Length: 150\n",
      "[Episode 135] Reward: 48.70 | Length: 66\n",
      "[Episode 136] Reward: 25.92 | Length: 150\n",
      "[Episode 137] Reward: 27.80 | Length: 37\n",
      "[Episode 138] Reward: 107.32 | Length: 150\n",
      "[Episode 139] Reward: 48.21 | Length: 68\n",
      "[Episode 140] Reward: 56.91 | Length: 82\n",
      "[Episode 141] Reward: 99.16 | Length: 150\n",
      "[Episode 142] Reward: 67.12 | Length: 89\n",
      "[Episode 143] Reward: 35.59 | Length: 48\n",
      "[Episode 144] Reward: 73.11 | Length: 96\n",
      "[Episode 145] Reward: 61.46 | Length: 84\n",
      "[Episode 146] Reward: 107.51 | Length: 147\n",
      "[Episode 147] Reward: 62.62 | Length: 87\n",
      "[Episode 148] Reward: 86.77 | Length: 150\n",
      "[Episode 149] Reward: 57.20 | Length: 73\n",
      "[Episode 150] Reward: 30.26 | Length: 44\n",
      "[Episode 151] Reward: 108.24 | Length: 150\n",
      "[Episode 152] Reward: 111.00 | Length: 150\n",
      "[Episode 153] Reward: 32.96 | Length: 44\n",
      "[Episode 154] Reward: 109.16 | Length: 150\n",
      "[Episode 155] Reward: 117.60 | Length: 150\n",
      "[Episode 156] Reward: 32.30 | Length: 49\n",
      "[Episode 157] Reward: 111.27 | Length: 150\n",
      "[Episode 158] Reward: 67.24 | Length: 89\n",
      "[Episode 159] Reward: 110.28 | Length: 150\n",
      "[Episode 160] Reward: 108.81 | Length: 150\n",
      "[Episode 161] Reward: 37.27 | Length: 50\n",
      "[Episode 162] Reward: 108.99 | Length: 150\n",
      "[Episode 163] Reward: 105.04 | Length: 150\n",
      "[Episode 164] Reward: 31.62 | Length: 40\n",
      "[Episode 165] Reward: 97.66 | Length: 141\n",
      "[Episode 166] Reward: 37.85 | Length: 150\n",
      "[Episode 167] Reward: 114.80 | Length: 150\n",
      "[Episode 168] Reward: 32.56 | Length: 46\n",
      "[Episode 169] Reward: 39.72 | Length: 52\n",
      "[Episode 170] Reward: 67.77 | Length: 93\n",
      "[Episode 171] Reward: 18.66 | Length: 27\n",
      "[Episode 172] Reward: 106.27 | Length: 150\n",
      "[Episode 173] Reward: 112.43 | Length: 150\n",
      "[Episode 174] Reward: 102.57 | Length: 150\n",
      "[Episode 175] Reward: 101.97 | Length: 150\n",
      "[Episode 176] Reward: 51.42 | Length: 68\n",
      "[Episode 177] Reward: 96.06 | Length: 150\n",
      "[Episode 178] Reward: 41.75 | Length: 150\n",
      "[Episode 179] Reward: 50.19 | Length: 83\n",
      "[Episode 180] Reward: 106.62 | Length: 150\n",
      "[Episode 181] Reward: 44.67 | Length: 62\n",
      "[Episode 182] Reward: 108.85 | Length: 150\n",
      "[Episode 183] Reward: 32.47 | Length: 46\n",
      "[Episode 184] Reward: 107.30 | Length: 150\n",
      "[Episode 185] Reward: 68.69 | Length: 88\n",
      "[Episode 186] Reward: 106.24 | Length: 150\n",
      "[Episode 187] Reward: 118.65 | Length: 150\n",
      "[Episode 188] Reward: 110.56 | Length: 150\n",
      "[Episode 189] Reward: 29.41 | Length: 39\n",
      "[Episode 190] Reward: 19.36 | Length: 27\n",
      "[Episode 191] Reward: 50.37 | Length: 66\n",
      "[Episode 192] Reward: 21.66 | Length: 150\n",
      "[Episode 193] Reward: 37.64 | Length: 52\n",
      "[Episode 194] Reward: 62.54 | Length: 86\n",
      "[Episode 195] Reward: 81.23 | Length: 112\n",
      "[Episode 196] Reward: 116.41 | Length: 150\n",
      "[Episode 197] Reward: 101.37 | Length: 150\n",
      "[Episode 198] Reward: 110.09 | Length: 150\n",
      "[Episode 199] Reward: 102.87 | Length: 150\n",
      "[Episode 200] Reward: 96.70 | Length: 150\n",
      "[Eval Episode 01] steps: 98  reward: 89.27\n",
      "[Eval Episode 02] steps: 97  reward: 87.79\n",
      "[Eval Episode 03] steps: 107  reward: 97.14\n",
      "[Eval Episode 04] steps: 200  reward: 183.16\n",
      "[Eval Episode 05] steps: 124  reward: 115.23\n",
      "[Eval Episode 06] steps: 70  reward: 64.97\n",
      "[Eval Episode 07] steps: 52  reward: 47.08\n",
      "[Eval Episode 08] steps: 200  reward: 182.84\n",
      "[Eval Episode 09] steps: 30  reward: 26.94\n",
      "[Eval Episode 10] steps: 24  reward: 21.41\n",
      "[Eval Episode 01] steps: 200  reward: 10.82\n",
      "[Eval Episode 02] steps: 200  reward: 8.28\n",
      "[Eval Episode 03] steps: 200  reward: 5.21\n",
      "[Eval Episode 04] steps: 200  reward: 8.39\n",
      "[Eval Episode 05] steps: 200  reward: 10.55\n",
      "[Eval Episode 06] steps: 200  reward: 2.74\n",
      "[Eval Episode 07] steps: 200  reward: 8.34\n",
      "[Eval Episode 08] steps: 200  reward: 7.81\n",
      "[Eval Episode 09] steps: 200  reward: 6.60\n",
      "[Eval Episode 10] steps: 200  reward: 11.84\n",
      "DDPG avg reward: 91.58 vs random avg reward: 8.06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(agent, env, episodes=10, max_steps = 200, random_agent=False):\n",
    "    total_reward = 0.0\n",
    "    for ep in range(1, episodes + 1):\n",
    "        # gymnasium 0.26+ : reset → (obs, info)\n",
    "        out = env.reset()\n",
    "        state = out[0] if isinstance(out, tuple) else out\n",
    "        if isinstance(state, dict):\n",
    "            state = state[\"observation\"]\n",
    "\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        for step in range(1, max_steps + 1):\n",
    "            if random_agent:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(state, noise=False)\n",
    "\n",
    "            step_out = env.step(action)\n",
    "            # gymnasium.step() → (obs, reward, terminated, truncated, info)\n",
    "            if len(step_out) == 5:\n",
    "                nxt, reward, term, trunc, _ = step_out\n",
    "                done = term or trunc\n",
    "            else:\n",
    "                nxt, reward, done, _ = step_out\n",
    "\n",
    "            if isinstance(nxt, tuple):\n",
    "                nxt = nxt[0]\n",
    "            if isinstance(nxt, dict):\n",
    "                nxt = nxt[\"observation\"]\n",
    "\n",
    "            state = nxt\n",
    "            ep_reward += reward\n",
    "\n",
    "            # Arrêt si on a atteint done ou max_steps\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f\"[Eval Episode {ep:02d}] steps: {step}  reward: {ep_reward:.2f}\")\n",
    "        total_reward += ep_reward\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent = DDPGAgent(state_dim, action_dim, act_max, device)\n",
    "    rewards = train(agent, env)\n",
    "    # test final\n",
    "    avg_ddpg = evaluate(agent, env, random_agent=False)\n",
    "    avg_rand = evaluate(agent, env, random_agent=True)\n",
    "    print(f\"DDPG avg reward: {avg_ddpg:.2f} vs random avg reward: {avg_rand:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91082c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this command in your terminal to see with tensorboard the learning : tensorboard --logdir runs_event_racetrack/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
