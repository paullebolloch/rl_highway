{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03333a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "import highway_env\n",
    "import pyvirtualdisplay\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fonctions de visualisation ###\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "import os\n",
    "if os.name != 'nt':\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(1024, 768))\n",
    "    display.start()\n",
    "\n",
    "from matplotlib import pyplot as plt, animation\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def create_anim(frames, dpi, fps):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    def setup():\n",
    "        plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n",
    "    return anim\n",
    "\n",
    "def display_anim(frames, dpi=72, fps=50):\n",
    "    anim = create_anim(frames, dpi, fps)\n",
    "    return anim.to_jshtml()\n",
    "\n",
    "def save_anim(frames, filename, dpi=72, fps=50):\n",
    "    anim = create_anim(frames, dpi, fps)\n",
    "    # FORCER LE BACKEND FFMPEG (nécessite ffmpeg installé sur votre système)\n",
    "    writer = FFMpegWriter(fps=fps)\n",
    "    anim.save(filename, writer=writer, dpi=dpi)\n",
    "\n",
    "\n",
    "class trigger:\n",
    "    def __init__(self):\n",
    "        self._trigger = True\n",
    "\n",
    "    def __call__(self, e):\n",
    "        return self._trigger\n",
    "\n",
    "    def set(self, t):\n",
    "        self._trigger = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f28840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment configuration\n",
    "with open(\"config_part1.pkl\", \"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "\n",
    "env = gym.make('highway-v0', render_mode=\"rgb_array\")\n",
    "env.unwrapped.configure(config)\n",
    "raw_env = env.unwrapped\n",
    "def make_env(render_mode='rgb_array'):\n",
    "    # crée l'env avec render_mode\n",
    "    env = gym.make(\"highway-fast-v0\", config=config, render_mode=render_mode)\n",
    "    # enregistre automatiquement les stats (reward, longueur, success…)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    # filme tous les épisodes dans videos/ avec préfixe \"episode\"\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=\"videos/\",\n",
    "        episode_trigger=lambda episode_id: episode_id % 10 == 0,\n",
    "        name_prefix=\"episode\"\n",
    "    )\n",
    "    return env\n",
    "idle_idx = env.unwrapped.action_type.actions_indexes[\"IDLE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'extraction des frames\n",
    "frames = []\n",
    "# gym.reset() renvoie (obs, info)\n",
    "reset_ret = env.reset()\n",
    "obs, info = reset_ret if isinstance(reset_ret, tuple) else (reset_ret, {})\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "for _ in range(150):\n",
    "    frame = env.render()           \n",
    "    frames.append(frame)\n",
    "\n",
    "    step_ret = env.step(idle_idx)\n",
    "    if len(step_ret) == 5:\n",
    "        obs, reward, done, truncated, info = step_ret\n",
    "    else:\n",
    "        obs, reward, done, info = step_ret\n",
    "\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "# 5. Nettoyage\n",
    "env.close()\n",
    "\n",
    "# 6. Affichage de l'animation dans le notebook\n",
    "HTML(display_anim(frames, dpi=72, fps=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24786d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "buffer_capacity = 10000\n",
    "update_target_every = 50  # steps before target network update\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 1e-4       # decrease per step\n",
    "learning_rate = 1e-3\n",
    "num_episodes = 500\n",
    "max_steps = int(config.get(\"duration\", 60) * config.get(\"policy_frequency\", 1))\n",
    "\n",
    "# Définition du writer pour TensorBoard\n",
    "writer = SummaryWriter(log_dir='highway_dqn/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eeea80",
   "metadata": {},
   "source": [
    "### Définition des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    def push(self, state, action, reward, done, next_state):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, done, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, k=batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Neural Network for Q-values\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, obs_shape, n_actions, writer = None):\n",
    "        self.obs_dim = int(np.prod(obs_shape))\n",
    "        self.n_actions = n_actions\n",
    "        self.q_net = Net(self.obs_dim, 128, n_actions)\n",
    "        self.target_net = Net(self.obs_dim, 128, n_actions)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = epsilon_start\n",
    "        self.losses = []\n",
    "        self.writer = writer # stockage du writer pour tensorboard\n",
    "\n",
    "    def get_action(self, state, use_epsilon=True):\n",
    "        if use_epsilon and random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        state_v = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
    "        q_vals = self.q_net(state_v)\n",
    "        return int(torch.argmax(q_vals, dim=1).item())\n",
    "\n",
    "    def append_and_learn(self, state, action, reward, done, next_state):\n",
    "        self.buffer.push(state, action, reward, done, next_state)\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        batch = self.buffer.sample(batch_size)\n",
    "        states, actions, rewards, dones, next_states = zip(*batch)\n",
    "        states_arr = np.stack(states).astype(np.float32)\n",
    "        next_states_arr = np.stack(next_states).astype(np.float32)\n",
    "        states_v = torch.from_numpy(states_arr).view(batch_size, -1)\n",
    "        next_states_v = torch.from_numpy(next_states_arr).view(batch_size, -1)\n",
    "        actions_v = torch.tensor(actions, dtype=torch.int64).view(batch_size, 1)\n",
    "        rewards_v = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones_v = torch.tensor(dones, dtype=torch.float32)\n",
    "        q_values = self.q_net(states_v).gather(1, actions_v).squeeze(1)\n",
    "        next_q_values = self.target_net(next_states_v).max(1)[0]\n",
    "        expected_q = rewards_v + gamma * next_q_values * (1 - dones_v)\n",
    "        loss = nn.MSELoss()(q_values, expected_q.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.losses.append(loss.item())\n",
    "        # ─── Logging conditionnel dans TensorBoard ───\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(\"train/loss\", loss.item(), self.steps_done)\n",
    "        if self.steps_done % update_target_every == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.epsilon = max(epsilon_end, self.epsilon - epsilon_decay)\n",
    "        self.steps_done += 1\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_agent(agent, episodes):\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        total_reward = 0.0\n",
    "        for t in range(max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "            done = terminated or truncated\n",
    "            agent.append_and_learn(state, action, reward, done, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep+1}/{episodes} - Reward: {total_reward:.2f} - Epsilon: {agent.epsilon:.3f}\")\n",
    "                # ─── LOG DANS TENSORBOARD ───\n",
    "        writer.add_scalar(\"rollout/ep_rew_mean\", total_reward, ep)\n",
    "        writer.add_scalar(\"rollout/exploration_rate\", agent.epsilon, ep)\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dc9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate(model_path,\n",
    "                      episodes=5,\n",
    "                      max_steps=max_steps,\n",
    "                      fps=30,\n",
    "                      video_path='highway_dqn/eval.mp4'\n",
    "                      ):\n",
    "    # --- 1. Création de l'env d'évaluation en mode rgb_array\n",
    "    eval_env = make_env(render_mode='rgb_array')\n",
    "\n",
    "    # --- 2. Instanciation et chargement de l'agent\n",
    "    sample_obs, _ = eval_env.reset()\n",
    "    obs_shape = np.array(sample_obs, dtype=np.float32).shape\n",
    "    eval_agent = DQNAgent(obs_shape, eval_env.action_space.n)\n",
    "    eval_agent.q_net.load_state_dict(torch.load(model_path))\n",
    "    eval_agent.q_net.eval()\n",
    "\n",
    "    # --- 3. Boucle d'évaluation + collecte des frames\n",
    "    all_frames = []\n",
    "    for ep in range(episodes):\n",
    "        state, _ = eval_env.reset()\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        done = False\n",
    "        t = 0\n",
    "        while not done and t < max_steps:\n",
    "            frame = eval_env.render()           # renvoie un ndarray RGB\n",
    "            all_frames.append(frame)\n",
    "            action = eval_agent.get_action(state, use_epsilon=False)\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            state = np.array(next_state, dtype=np.float32)\n",
    "            done = terminated or truncated\n",
    "            t += 1\n",
    "\n",
    "    # --- 4. Fermeture de l'env\n",
    "    eval_env.close()\n",
    "\n",
    "    # --- 5. Sauvegarde de la vidéo MP4\n",
    "    #    (utilise votre fonction créée en première cellule)\n",
    "    save_anim(all_frames, video_path, dpi=72, fps=fps)\n",
    "\n",
    "    print(f\"Vidéo d'évaluation sauvegardée dans : {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Instanciation\n",
    "env = make_env()\n",
    "sample_obs, _ = env.reset()\n",
    "max_steps = int(config.get(\"duration\", 60) * config.get(\"policy_frequency\", 1))\n",
    "agent = DQNAgent(obs_shape=np.array(sample_obs).shape,\n",
    "                 n_actions=env.action_space.n,\n",
    "                 writer=writer)\n",
    "\n",
    "# 2) Entraînement\n",
    "train_agent(agent, num_episodes)\n",
    "\n",
    "# 3) Sauvegarde du modèle\n",
    "torch.save(agent.q_net.state_dict(), \"highway_dqn/model.pt\")\n",
    "\n",
    "# 4) Évaluation et vidéo finale\n",
    "load_and_evaluate(\"highway_dqn/model.pt\", episodes=20,\n",
    "                  max_steps=max_steps,\n",
    "                  fps=30,\n",
    "                  video_path=\"highway_dqn/eval.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4eab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!taskkill /PID 3780 /F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=highway_dqn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84949a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
